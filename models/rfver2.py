import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.metrics import mean_absolute_error
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from google.colab import drive
drive.mount('/content/drive')

# í•œê¸€ í°íŠ¸ ì„¤ì •
font_path = '/content/drive/MyDrive/SeoulNamsanC.otf'
font_prop = fm.FontProperties(fname=font_path)
plt.style.use('dark_background')

# --- 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ---
print("ë°ì´í„° ë¡œë”© ì¤‘...")
train_df = pd.read_csv('/content/drive/MyDrive/lgaimers/train.csv')

# ìŒìˆ˜ë°ì´í„° ì²˜ë¦¬
train_df['ë§¤ì¶œìˆ˜ëŸ‰'] = train_df['ë§¤ì¶œìˆ˜ëŸ‰'].clip(lower=0)
# 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' ì»¬ëŸ¼ì— 'ë‹¨ì²´'ê°€ í¬í•¨ëœ í–‰ë§Œ ë‚¨ê¹€
train_df = train_df[train_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.contains('ë‹¨ì²´')]

# ì…ë ¥ ë° ì˜ˆì¸¡ êµ¬ê°„ ë°ì´í„° ë¡œë”© + ì´ìƒì¹˜ í•„í„°ë§ (ë‹¨ì²´ì‹ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµ ë° ì˜ˆì¸¡)
test_df = pd.read_csv('/content/drive/MyDrive/lgaimers/TEST_00.csv')
test_df = test_df[test_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.contains('ë‹¨ì²´')]

train_df['ì˜ì—…ì¼ì'] = pd.to_datetime(train_df['ì˜ì—…ì¼ì'])
test_df['ì˜ì—…ì¼ì'] = pd.to_datetime(test_df['ì˜ì—…ì¼ì'])

print("ë°ì´í„° í”¼ë²— í…Œì´ë¸” ìƒì„± ì¤‘...")
sales_df = train_df.pivot_table(index='ì˜ì—…ì¼ì', columns='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', values='ë§¤ì¶œìˆ˜ëŸ‰', fill_value=0)
sales_df = sales_df.asfreq('D', fill_value=0)

time_features_df = pd.DataFrame(index=sales_df.index)
time_features_df['month'] = sales_df.index.month
time_features_df['dayofweek'] = sales_df.index.dayofweek

time_features_df['is_weekend'] = time_features_df['dayofweek'].isin([5, 6]).astype(int)
dow_onehot = pd.get_dummies(time_features_df['dayofweek'], prefix='dow')
time_features_df = pd.concat([time_features_df, dow_onehot], axis=1)
BASE_COLS = ['is_weekend','month'] + [f'dow_{d}' for d in range(7)]

time_features_df['is_weekend_dup'] = time_features_df['is_weekend']
BASE_COLS.append('is_weekend_dup')
print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.")


# --- 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ---
class BlockBootstrapRandomForest:
    """
    Random Forest Regressor with a custom blocked bootstrap sampling method.
    """
    def __init__(self, n_estimators=100, max_depth=10, block_size=28, random_state=42):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.block_size = block_size
        self.random_state = random_state
        self.trees = []
        np.random.seed(self.random_state)

    def _blocked_bootstrap_indices(self, n_samples):
        num_blocks = n_samples - self.block_size + 1
        if num_blocks <= 0:
            return np.random.choice(n_samples, size=n_samples, replace=True)
        block_starts = np.arange(num_blocks)
        random_block_starts = np.random.choice(block_starts, size=num_blocks, replace=True)
        bootstrap_indices = []
        for start in random_block_starts:
            bootstrap_indices.extend(np.arange(start, start + self.block_size))
        return np.array(bootstrap_indices[:n_samples])

    def fit(self, X, y):
        self.trees = []
        n_samples = X.shape[0]
        for i in tqdm(range(self.n_estimators), desc=f"  - {self.n_estimators}ê°œ ë‚˜ë¬´ í•™ìŠµ"):
            boot_indices = self._blocked_bootstrap_indices(n_samples)
            if len(boot_indices) == 0: continue
            X_boot, y_boot = X[boot_indices], y[boot_indices]
            tree = DecisionTreeRegressor(
                max_depth=self.max_depth,
                random_state=i
                )
            tree.fit(X_boot, y_boot)
            self.trees.append(tree)
        return self

    def predict(self, X):
        if not self.trees:
            raise RuntimeError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        X = X.reshape(1, -1)
        predictions = np.array([tree.predict(X) for tree in self.trees])
        return np.mean(predictions, axis=0).flatten()


# --- 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° í•™ìŠµ/ì˜ˆì¸¡ í•¨ìˆ˜ ---

def create_sliding_window_with_stats_and_lags(
    sales_data: np.ndarray,
    time_features: pd.DataFrame,
    look_back: int,
    horizon: int
):
    """
    ë§¤ì¶œìˆ˜ëŸ‰, í†µê³„ëŸ‰, ë˜ê·¸ í”¼ì²˜, ì‹œê°„ íŠ¹ì„±ì„ ê²°í•©í•˜ì—¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±.
    """
    X, y_raw = [], []

    for i in range(len(sales_data) - look_back - horizon + 1):
        # ìœˆë„ìš° êµ¬ê°„ì˜ ë§¤ì¶œ ë° í†µê³„ëŸ‰
        window_sales = sales_data[i : i + look_back]
        mean_lag = np.mean(window_sales)
        std_lag = np.std(window_sales)

        # ë˜ê·¸ í”¼ì²˜ ì¶”ê°€ (i + look_back ëŠ” í˜„ì¬ ì‹œì ì˜ ì¸ë±ìŠ¤)
        lag_1 = sales_data[i + look_back - 1] if i + look_back - 1 >= 0 else 0
        lag_7 = sales_data[i + look_back - 7] if i + look_back - 7 >= 0 else 0
        lag_14 = sales_data[i + look_back - 14] if i + look_back - 14 >= 0 else 0

        # ì˜ˆì¸¡ êµ¬ê°„ì˜ ë¯¸ë˜ ë‚ ì§œ íŠ¹ì„±
        future_time = (
            time_features.iloc[i + look_back : i + look_back + horizon][BASE_COLS]
            .values
            .flatten()
        )

        # ëª¨ë“  í”¼ì²˜ë¥¼ ê²°í•© (ë˜ê·¸ í”¼ì²˜ ì¶”ê°€)
        feats = np.concatenate([window_sales, [mean_lag, std_lag, lag_1, lag_7, lag_14], future_time])
        X.append(feats)

        # ì •ë‹µ(y) -> ë‹¤ìŒ horizonì¼ ë§¤ì¶œ
        y_raw.append(sales_data[i + look_back : i + look_back + horizon])
    
    return np.asarray(X), np.asarray(y_raw)


def two_stage_train_and_predict(
    historical_series: pd.Series,
    time_features: pd.DataFrame,
    test_sales_input: np.ndarray,
    test_time_input: np.ndarray
):
    LOOK_BACK = 28
    HORIZON = 7
    BLOCK_SIZE = 28

    print(f"\n--- '{historical_series.name}' 2ë‹¨ê³„ ì˜ˆì¸¡ ëª¨ë¸ ì‹œì‘ ---")

    # 1. í•™ìŠµ ë°ì´í„° ìƒì„± (ë˜ê·¸ í”¼ì²˜ í¬í•¨)
    X_train_raw, y_train_raw = create_sliding_window_with_stats_and_lags(
        historical_series.values, time_features, LOOK_BACK, HORIZON
    )
    if len(X_train_raw) < BLOCK_SIZE:
        print("  - ë°ì´í„° ë¶€ì¡±. 0ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
        return np.zeros(HORIZON, dtype=int)

    # 1ë‹¨ê³„: ë§¤ì¶œ ë°œìƒ ì—¬ë¶€ (ë¶„ë¥˜) ëª¨ë¸ í•™ìŠµ
    print("  - 1ë‹¨ê³„: ë§¤ì¶œ ë°œìƒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ...")
    y_train_cls = np.clip(y_train_raw, 0, 1).astype(int)
    
    # y_train_clsëŠ” 7ì¼ì¹˜ ì˜ˆì¸¡ì´ë¯€ë¡œ, 7ê°œì˜ íŠ¸ë¦¬ ëª¨ë¸ì„ í•™ìŠµ
    cls_models = []
    for i in range(HORIZON):
        model = DecisionTreeClassifier(max_depth=5, random_state=i)
        model.fit(X_train_raw, y_train_cls[:, i])
        cls_models.append(model)
        
    # 2ë‹¨ê³„: ë§¤ì¶œ ìˆ˜ëŸ‰ (íšŒê·€) ëª¨ë¸ í•™ìŠµ
    print("  - 2ë‹¨ê³„: ë§¤ì¶œ ìˆ˜ëŸ‰ íšŒê·€ ëª¨ë¸ í•™ìŠµ...")
    # ë§¤ì¶œì´ 0ì´ ì•„ë‹Œ ë°ì´í„°ë§Œ í•„í„°ë§
    non_zero_indices = np.where(np.any(y_train_raw > 0, axis=1))[0]
    X_train_reg = X_train_raw[non_zero_indices]
    y_train_reg = y_train_raw[non_zero_indices]

    reg_model = BlockBootstrapRandomForest(
        ### ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        n_estimators=100, max_depth=10, block_size=BLOCK_SIZE, random_state=42
    )
    if len(X_train_reg) < BLOCK_SIZE:
        print("  - 2ë‹¨ê³„ íšŒê·€ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±. 1ë‹¨ê³„ ì˜ˆì¸¡ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        reg_model_is_trained = False
    else:
        reg_model.fit(X_train_reg, y_train_reg)
        reg_model_is_trained = True

    # 3. ì˜ˆì¸¡ ê³¼ì •
    print("  - ë¯¸ë˜ 7ì¼ ì˜ˆì¸¡ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ë˜ê·¸ í”¼ì²˜ í¬í•¨)
    mean_test = np.mean(test_sales_input)
    std_test  = np.std(test_sales_input)
    
    lag_1_test = test_sales_input[-1]
    lag_7_test = test_sales_input[-7]
    lag_14_test = test_sales_input[-14]

    combined_test_input = np.concatenate([test_sales_input, [mean_test, std_test, lag_1_test, lag_7_test, lag_14_test], test_time_input.flatten()])

    final_predictions = np.zeros(HORIZON, dtype=int)
    
    for i in range(HORIZON):
        # 1ë‹¨ê³„: ë§¤ì¶œ ë°œìƒ ì—¬ë¶€ ì˜ˆì¸¡
        cls_pred = cls_models[i].predict(combined_test_input.reshape(1, -1))[0]

        if cls_pred == 1 and reg_model_is_trained:
            # 2ë‹¨ê³„: ë§¤ì¶œì´ ë°œìƒí•œë‹¤ê³  ì˜ˆì¸¡ë˜ë©´, ìˆ˜ëŸ‰ ì˜ˆì¸¡
            reg_pred = reg_model.predict(combined_test_input)
            final_predictions[i] = reg_pred[i]
    
    final_predictions = np.clip(np.round(final_predictions), 0, None).astype(int)

    print(f"--- '{historical_series.name}' ì˜ˆì¸¡ ì™„ë£Œ ---")
    return final_predictions


# --- 4. â˜…â˜…â˜…â˜…â˜… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ â˜…â˜…â˜…â˜…â˜…

all_results = {}
test_df_grouped = test_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')

# test_df_groupedì´ (a,b) í˜•íƒœì˜ íŠœí”Œì„ ê³„ì† ë°˜í™˜í•  ë•Œ ê°ê° ë‘ ë³€ìˆ˜ì— ìë™ìœ¼ë¡œ í• ë‹¹í•œë‹¤. ë°˜í™˜ë˜ëŠ” íŠœí”Œì˜ í˜•íƒœëŠ” ì—¬ê¸°ì„œ (ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…, í•´ë‹¹ í’ˆëª©ì˜ ë°ì´í„°í”„ë ˆì„)
for item, group in test_df_grouped:
    if item in sales_df.columns:
        # 1. í›ˆë ¨ ë°ì´í„°: train.csvì—ì„œ í•´ë‹¹ ì•„ì´í…œì˜ ì „ì²´ ì‹œê³„ì—´
        full_historical_series = sales_df[item]

        # 2. â˜…â˜…â˜…â˜…â˜… ë°ì´í„° ë¶„í•  â˜…â˜…â˜…â˜…â˜…
        # í›ˆë ¨ ë°ì´í„°ì—ì„œ ë§ˆì§€ë§‰ 7ì¼ ë¶„ë¦¬
        adjusted_historical_series = full_historical_series[:-7]
        last_7_days_of_train = full_historical_series[-7:].values

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ 21ì¼(ì…ë ¥ìš©)ê³¼ 7ì¼(ê²€ì¦ìš©) ë¶„ë¦¬
        sorted_group = group.sort_values('ì˜ì—…ì¼ì')
        first_21_days_of_test = sorted_group['ë§¤ì¶œìˆ˜ëŸ‰'].iloc[:21].values
        actual_values = sorted_group['ë§¤ì¶œìˆ˜ëŸ‰'].iloc[21:].values # ì‹¤ì œê°’ (ì •ë‹µ)

        if len(first_21_days_of_test) != 21 or len(actual_values) != 7:
            print(f"ê²½ê³ : '{item}'ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  ë¶ˆê°€. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # 3. â˜…â˜…â˜…â˜…â˜… ê²€ì¦ìš© ì…ë ¥ ë°ì´í„° ìƒì„± â˜…â˜…â˜…â˜…â˜…
        validation_input_sales = np.concatenate([last_7_days_of_train, first_21_days_of_test])

        # 4. ì˜ˆì¸¡ì— í•„ìš”í•œ ë¯¸ë˜ 7ì¼(ê²€ì¦ ê¸°ê°„)ì˜ ì‹œê°„ íŠ¹ì„± ìƒì„±
        validation_dates = sorted_group['ì˜ì—…ì¼ì'].iloc[21:]
        future_time_features = pd.DataFrame(index=validation_dates.index)
        future_time_features['month'] = validation_dates.dt.month
        future_time_features['dayofweek'] = validation_dates.dt.dayofweek
        future_time_features['is_weekend'] = future_time_features['dayofweek'].isin([5, 6]).astype(int)
        future_time_features['is_weekend_dup'] = future_time_features['is_weekend']
        ### ìš”ì¼ë³€ìˆ˜ë¥¼ ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ë§¤í•‘
        dow_onehot_future = pd.get_dummies(future_time_features['dayofweek'], prefix='dow')
        future_time_features = pd.concat([future_time_features, dow_onehot_future], axis=1)

        validation_time_input = future_time_features[BASE_COLS].values

        # 5. ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ ì‹¤í–‰
        prediction = two_stage_train_and_predict(
            adjusted_historical_series,
            time_features_df,
            validation_input_sales,
            validation_time_input
        )

        # ê²°ê³¼ ì €ì¥
        all_results[item] = {'prediction': prediction, 'actual': actual_values}

    else:
        print(f"ê²½ê³ : '{item}' í•­ëª©ì€ í›ˆë ¨ ë°ì´í„°ì— ì—†ì–´ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")

# â˜…â˜…â˜…â˜…â˜… ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ ì‹œê°í™” â˜…â˜…â˜…â˜…â˜…

all_predictions_flat = []
all_actuals_flat = []

for item, result in all_results.items():
    all_predictions_flat.extend(result['prediction'])
    all_actuals_flat.extend(result['actual'])

# ì „ì²´ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨(MAE) ê³„ì‚°
if all_actuals_flat:
    total_mae = mean_absolute_error(all_actuals_flat, all_predictions_flat)
    print("\n\n" + "="*50)
    print(f"ğŸ“Š ì „ì²´ í’ˆëª©ì— ëŒ€í•œ ìµœì¢… ì˜ˆì¸¡ ì„±ëŠ¥ (MAE): {total_mae:.4f}")
    print("="*50 + "\n")